#!/usr/bin/env python3
"""
NLPæ¨¡å‹ä¸‹è½½è„šæœ¬
Download NLP Models Script
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def print_colored(message: str, color: str = "white"):
    """æ‰“å°å½©è‰²æ¶ˆæ¯"""
    colors = {
        "red": "\033[91m",
        "green": "\033[92m",
        "yellow": "\033[93m",
        "blue": "\033[94m",
        "purple": "\033[95m",
        "cyan": "\033[96m",
        "white": "\033[97m",
        "reset": "\033[0m"
    }
    print(f"{colors.get(color, colors['white'])}{message}{colors['reset']}")


def download_nltk_data():
    """ä¸‹è½½NLTKæ•°æ®"""
    print_colored("ğŸ”¤ ä¸‹è½½NLTKæ•°æ®...", "cyan")

    try:
        import nltk

        # å¿…éœ€çš„NLTKæ•°æ®åŒ…
        required_packages = [
            'punkt',  # åˆ†è¯å™¨
            'stopwords',  # åœç”¨è¯
            'wordnet',   # è¯æ±‡æ•°æ®åº“
        ]

        # å¯é€‰çš„NLTKæ•°æ®åŒ…ï¼ˆç”¨äºæ›´é«˜çº§çš„NLPåŠŸèƒ½ï¼‰
        optional_packages = [
            'averaged_perceptron_tagger',  # è¯æ€§æ ‡æ³¨
            'maxent_ne_chunker',          # å‘½åå®ä½“è¯†åˆ«
            'words',                       # å•è¯åˆ—è¡¨
        ]

        print("  ä¸‹è½½å¿…éœ€åŒ…...")
        for package in required_packages:
            try:
                nltk.data.find(f'tokenizers/{package}')
                print(f"    âœ… {package} - å·²å­˜åœ¨")
            except LookupError:
                print(f"    ğŸ“¥ ä¸‹è½½ {package}...")
                nltk.download(package)

        print("  ä¸‹è½½å¯é€‰åŒ…...")
        for package in optional_packages:
            try:
                nltk.data.find(f'taggers/{package}')
                print(f"    âœ… {package} - å·²å­˜åœ¨")
            except LookupError:
                print(f"    ğŸ“¥ ä¸‹è½½ {package}...")
                nltk.download(package)

        print_colored("âœ… NLTKæ•°æ®ä¸‹è½½å®Œæˆ!", "green")
        return True

    except Exception as e:
        print_colored(f"âŒ NLTKæ•°æ®ä¸‹è½½å¤±è´¥: {e}", "red")
        return False


def download_spacy_models():
    """ä¸‹è½½SpaCyæ¨¡å‹"""
    print_colored("ğŸ§  ä¸‹è½½SpaCyæ¨¡å‹...", "cyan")

    try:
        import spacy

        # æ”¯æŒçš„SpaCyæ¨¡å‹
        models = [
            ('zh_core_web_sm', 'ä¸­æ–‡å°å‹æ¨¡å‹'),
            ('en_core_web_sm', 'è‹±æ–‡å°å‹æ¨¡å‹'),
            ('zh_core_web_md', 'ä¸­æ–‡ä¸­å‹æ¨¡å‹'),
            ('en_core_web_md', 'è‹±æ–‡ä¸­å‹æ¨¡å‹'),
        ]

        successful_downloads = 0
        for model_id, description in models:
            try:
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
                spacy.load(model_id)
                print(f"    âœ… {model_id} ({description}) - å·²å­˜åœ¨")
                successful_downloads += 1
            except OSError:
                print(f"    ğŸ“¥ ä¸‹è½½ {model_id} ({description})...")
                try:
                    spacy.cli.download(model_id)
                    successful_downloads += 1
                    print(f"    âœ… {model_id} - ä¸‹è½½æˆåŠŸ")
                except Exception as download_error:
                    print(f"    âŒ {model_id} - ä¸‹è½½å¤±è´¥: {download_error}")

        if successful_downloads > 0:
            print_colored(f"âœ… SpaCyæ¨¡å‹ä¸‹è½½å®Œæˆ! ({successful_downloads}/{len(models)} ä¸ªæ¨¡å‹)", "green")
        else:
            print_colored("âš ï¸  æ²¡æœ‰ä¸‹è½½SpaCyæ¨¡å‹ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™", "yellow")

        return successful_downloads > 0

    except Exception as e:
        print_colored(f"âŒ SpaCyæ¨¡å‹ä¸‹è½½å¤±è´¥: {e}", "red")
        return False


def check_huggingface_models():
    """æ£€æŸ¥HuggingFaceæ¨¡å‹"""
    print_colored("ğŸ¤— æ£€æŸ¥HuggingFaceæ¨¡å‹...", "cyan")

    try:
        from transformers import AutoTokenizer

        # æ£€æŸ¥å¸¸ç”¨çš„ä¸­æ–‡BERTæ¨¡å‹
        models_to_check = [
            'bert-base-chinese',
            'bert-base-multilingual-cased',
            'distilbert-base-multilingual-cased'
        ]

        successful_checks = 0
        for model_name in models_to_check:
            try:
                print(f"    ğŸ” æ£€æŸ¥æ¨¡å‹: {model_name}")
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                print(f"    âœ… {model_name} - å¯ç”¨")
                successful_checks += 1
            except Exception as e:
                print(f"    âŒ {model_name} - ä¸å¯ç”¨: {str(e)[:50]}...")

        if successful_checks > 0:
            print_colored(f"âœ… HuggingFaceæ¨¡å‹æ£€æŸ¥å®Œæˆ! ({successful_checks}/{len(models_to_check)} ä¸ªæ¨¡å‹å¯ç”¨)", "green")
        else:
            print_colored("âš ï¸  æ²¡æœ‰å¯ç”¨çš„HuggingFaceæ¨¡å‹", "yellow")

        return successful_checks > 0

    except Exception as e:
        print_colored(f"âŒ HuggingFaceæ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}", "red")
        return False


def create_model_config():
    """åˆ›å»ºæ¨¡å‹é…ç½®æ–‡ä»¶"""
    print_colored("âš™ï¸  åˆ›å»ºæ¨¡å‹é…ç½®æ–‡ä»¶...", "cyan")

    config_dir = project_root / "config"
    config_dir.mkdir(exist_ok=True)

    config_content = """# NLPæ¨¡å‹é…ç½®æ–‡ä»¶
# CPG2PVG-AI System NLP Model Configuration

# NLTKé…ç½®
NLTK_DATA_PATH = "nltk_data"
NLTK_LANGUAGE = "english"

# SpaCyé…ç½®
SPACY_MODEL_ZH = "zh_core_web_sm"  # ä¸­æ–‡æ¨¡å‹
SPACY_MODEL_EN = "en_core_web_sm"  # è‹±æ–‡æ¨¡å‹

# HuggingFaceé…ç½®
HF_MODEL_BASE = "bert-base-chinese"
HF_CACHE_DIR = "hf_cache"

# æ¨¡å‹ä¼˜å…ˆçº§
PREFERRED_LANGUAGE = "zh"  # zh: ä¸­æ–‡ä¼˜å…ˆ, en: è‹±æ–‡ä¼˜å…ˆ
FALLBACK_LANGUAGE = "en"

# ä¸‹è½½é…ç½®
AUTO_DOWNLOAD = true
SKIP_LARGE_MODELS = False

# æ€§èƒ½é…ç½®
MAX_CONCURRENT_DOWNLOADS = 3
DOWNLOAD_TIMEOUT = 300  # 5åˆ†é’Ÿ
"""

    config_file = config_dir / "nlp_models.py"

    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)

        print_colored(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}", "green")
        return True

    except Exception as e:
        print_colored(f"âŒ é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}", "red")
        return False


def create_download_script():
    """åˆ›å»ºæ¨¡å‹ä¸‹è½½è„šæœ¬"""
    script_content = '''#!/bin/bash
#!/bin/bash
# NLPæ¨¡å‹è‡ªåŠ¨ä¸‹è½½è„šæœ¬
# è‡ªåŠ¨ä¸‹è½½CPG2PVG-AIç³»ç»Ÿæ‰€éœ€çš„NLPæ¨¡å‹

set -e

echo "ğŸš€ å¼€å§‹ä¸‹è½½NLPæ¨¡å‹..."

# è®¾ç½®Pythonè·¯å¾„
PYTHON_PATH="$(dirname "$0")/../venv/bin"
if [ -d "$PYTHON_PATH" ]; then
    export PATH="$PYTHON_PATH:$PATH"
fi

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if [ -f "$(dirname "$0")/../venv/bin/activate" ]; then
    source "$(dirname "$0")/../venv/bin/activate"
fi

echo "ğŸ“¦ å®‰è£…Pythonä¾èµ–..."
pip install nltk spacy transformers tokenizers

echo "ğŸ”¤ ä¸‹è½½NLTKæ•°æ®..."
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet');"

echo "ğŸ§  ä¸‹è½½SpaCyæ¨¡å‹..."
python -c "import spacy; spacy.cli.download('zh_core_web_sm');"

echo "âœ… æ‰€æœ‰NLPæ¨¡å‹ä¸‹è½½å®Œæˆ!"
'''

    script_file = project_root / "scripts" / "download_nlp_models.sh"

    try:
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(script_content)

        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_file, 0o755)

        print_colored(f"âœ… ä¸‹è½½è„šæœ¬å·²åˆ›å»º: {script_file}", "green")
        print_colored("   è¿è¡Œå‘½ä»¤: bash scripts/download_nlp_models.sh", "cyan")
        return True

    except Exception as e:
        print_colored(f"âŒ ä¸‹è½½è„šæœ¬åˆ›å»ºå¤±è´¥: {e}", "red")
        return False


def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print_colored("ğŸ” æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...", "cyan")

    issues = []

    # æ£€æŸ¥Pythonç‰ˆæœ¬
    python_version = sys.version_info
    if python_version < (3, 8):
        issues.append(f"Pythonç‰ˆæœ¬è¿‡ä½: {python_version[0]}.{python_version[1]} (éœ€è¦ >= 3.8)")

    # æ£€æŸ¥ç½‘ç»œè¿æ¥
    try:
        import requests
        response = requests.get("https://pypi.org", timeout=10)
        if response.status_code != 200:
            issues.append("ç½‘ç»œè¿æ¥å¼‚å¸¸")
    except Exception:
        issues.append("ç½‘ç»œè¿æ¥æ£€æŸ¥å¤±è´¥")

    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    import shutil
    total, used, free = shutil.disk_usage(".")
    free_gb = free // (1024**3)
    if free_gb < 2:  # 2GB
        issues.append(f"ç£ç›˜ç©ºé—´ä¸è¶³: {free_gb}GB (æ¨èè‡³å°‘2GB)")

    if issues:
        print_colored("âš ï¸  ç³»ç»Ÿè¦æ±‚æ£€æŸ¥å¤±è´¥:", "yellow")
        for issue in issues:
            print(f"    - {issue}")
        return False

    print_colored("âœ… ç³»ç»Ÿè¦æ±‚æ£€æŸ¥é€šè¿‡!", "green")
    return True


def main():
    """ä¸»å‡½æ•°"""
    print_colored("ğŸ§  CPG2PVG-AI NLPæ¨¡å‹ä¸‹è½½è„šæœ¬", "blue")
    print("=" * 60)

    # æ£€æŸ¥ç³»ç»Ÿè¦æ±‚
    if not check_system_requirements():
        print_colored("âŒ ç³»ç»Ÿè¦æ±‚ä¸æ»¡è¶³ï¼Œè¯·è§£å†³åé‡è¯•", "red")
        return False

    print()

    success_count = 0
    total_checks = 4

    # 1. ä¸‹è½½NLTKæ•°æ®
    if download_nltk_data():
        success_count += 1

    # 2. ä¸‹è½½SpaCyæ¨¡å‹
    if download_spacy_models():
        success_count += 1

    # 3. æ£€æŸ¥HuggingFaceæ¨¡å‹
    if check_huggingface_models():
        success_count += 1

    # 4. åˆ›å»ºé…ç½®æ–‡ä»¶
    if create_model_config():
        success_count += 1

    # 5. åˆ›å»ºä¸‹è½½è„šæœ¬
    if create_download_script():
        success_count += 1

    print()
    print_colored("=" * 60, "blue")
    print_colored("ğŸ“Š ä¸‹è½½æ€»ç»“", "blue")
    print(f"  æˆåŠŸ: {success_count}/{total_checks} é¡¹")

    if success_count == total_checks:
        print_colored("ğŸ‰ æ‰€æœ‰NLPæ¨¡å‹ä¸‹è½½/é…ç½®å®Œæˆ!", "green")
        print()
        print_colored("ğŸ“‹ ä¸‹ä¸€æ­¥:", "cyan")
        print("  1. è¿è¡ŒåŒ»å­¦æ–‡æ¡£è§£æå™¨æµ‹è¯•:")
        print("     python scripts/test_medical_parser.py")
        print("  2. å¯åŠ¨å®Œæ•´ç³»ç»Ÿè®¾ç½®:")
        print("     python scripts/setup_complete_system.py")
        print("  3. å¼€å§‹ä½¿ç”¨åŒ»å­¦æ–‡æ¡£è§£æåŠŸèƒ½")
    else:
        print_colored(f"âš ï¸  éƒ¨åˆ†ä¸‹è½½å¤±è´¥ ({total_checks - success_count} é¡¹)", "yellow")
        print()
        print_colored("ğŸ”§ æ•…éšœæ’é™¤:", "yellow")
        print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("  - ç¡®ä¿å¯ä»¥è®¿é—® https://github.com")
        print("  - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")
        print()
        print("  2. æ£€æŸ¥æƒé™è®¾ç½®")
        print("  - ç¡®ä¿æœ‰å†™å…¥æƒé™")
        print("  - æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒæƒé™")
        print()
        print("  3. æ‰‹åŠ¨å®‰è£…ç¼ºå¤±æ¨¡å‹:")
        print("  - SpaCy: python -m spacy download zh_core_web_sm")
        print("  - NLTK: python -c \"import nltk; nltk.download('punkt')\"")

    return success_count == total_checks


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)