#!/usr/bin/env python3
"""
ä»£ç è´¨é‡åˆ†æå™¨
Code Quality Analyzer
"""

import ast
import os
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple
from collections import defaultdict

# é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent

class CodeQualityAnalyzer:
    """ä»£ç è´¨é‡åˆ†æå™¨"""

    def __init__(self):
        self.results = {
            'total_files': 0,
            'python_files': 0,
            'issues': defaultdict(list),
            'metrics': defaultdict(int),
            'complexity_scores': {},
            'type_annotation_coverage': 0,
            'docstring_coverage': 0
        }

    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªPythonæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # åŸºæœ¬ç»Ÿè®¡
            lines = content.split('\n')
            non_empty_lines = [line for line in lines if line.strip()]
            comment_lines = [line for line in lines if line.strip().startswith('#')]

            file_metrics = {
                'total_lines': len(lines),
                'code_lines': len(non_empty_lines),
                'comment_lines': len(comment_lines),
                'functions': 0,
                'classes': 0,
                'docstrings': 0,
                'type_annotations': 0
            }

            # ASTåˆ†æ
            try:
                tree = ast.parse(content)
                file_metrics.update(self._analyze_ast(tree))
            except SyntaxError as e:
                self.results['issues']['syntax_errors'].append(f"{file_path}: {e}")

            # ä»£ç è´¨é‡æ£€æŸ¥
            issues = self._check_code_quality(content, file_path)

            return {
                'file_path': str(file_path),
                'metrics': file_metrics,
                'issues': issues,
                'quality_score': self._calculate_quality_score(file_metrics, issues)
            }

        except Exception as e:
            self.results['issues']['analysis_errors'].append(f"{file_path}: {e}")
            return {}

    def _analyze_ast(self, tree: ast.AST) -> Dict[str, int]:
        """åˆ†æASTæ ‘"""
        metrics = {
            'functions': 0,
            'classes': 0,
            'docstrings': 0,
            'type_annotations': 0
        }

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                metrics['functions'] += 1
                if ast.get_docstring(node):
                    metrics['docstrings'] += 1
                if node.returns:
                    metrics['type_annotations'] += 1

                # æ£€æŸ¥å‚æ•°ç±»å‹æ³¨è§£
                for arg in node.args.args:
                    if arg.annotation:
                        metrics['type_annotations'] += 1

            elif isinstance(node, ast.AsyncFunctionDef):
                metrics['functions'] += 1
                if ast.get_docstring(node):
                    metrics['docstrings'] += 1
                if node.returns:
                    metrics['type_annotations'] += 1

            elif isinstance(node, ast.ClassDef):
                metrics['classes'] += 1
                if ast.get_docstring(node):
                    metrics['docstrings'] += 1

        return metrics

    def _check_code_quality(self, content: str, file_path: Path) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ä»£ç è´¨é‡é—®é¢˜"""
        issues = []
        lines = content.split('\n')

        # 1. æ£€æŸ¥è¡Œé•¿åº¦
        for i, line in enumerate(lines, 1):
            if len(line) > 120:
                issues.append({
                    'type': 'long_line',
                    'line': i,
                    'message': f'Line too long ({len(line)} > 120 characters)'
                })

        # 2. æ£€æŸ¥TODO/FIXME
        for i, line in enumerate(lines, 1):
            if re.search(r'\b(TODO|FIXME|XXX|HACK)\b', line, re.IGNORECASE):
                issues.append({
                    'type': 'todo_comment',
                    'line': i,
                    'message': f'TODO/FIXME found: {line.strip()}'
                })

        # 3. æ£€æŸ¥å¯¼å…¥è¯­å¥é¡ºåº
        import_issues = self._check_imports(content)
        issues.extend(import_issues)

        # 4. æ£€æŸ¥ç©ºå‡½æ•°/ç±»
        if 'pass' in content and content.count('pass') > 5:
            issues.append({
                'type': 'many_passes',
                'message': f'Many pass statements ({content.count("pass")}) found'
            })

        # 5. æ£€æŸ¥å¼‚å¸¸å¤„ç†
        bare_except_count = len(re.findall(r'except\s*:', content))
        if bare_except_count > 0:
            issues.append({
                'type': 'bare_except',
                'message': f'Found {bare_except_count} bare except clauses'
            })

        return issues

    def _check_imports(self, content: str) -> List[Dict[str, Any]]:
        """æ£€æŸ¥å¯¼å…¥è¯­å¥"""
        issues = []
        lines = content.split('\n')
        imports = []

        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            if stripped.startswith(('import ', 'from ')):
                imports.append({'line': i, 'content': stripped})

        # æ£€æŸ¥å¯¼å…¥é¡ºåº (stdlib, third-party, local)
        if len(imports) > 3:
            stdlib_pattern = r'^(import (os|sys|json|re|time|datetime|uuid|pathlib|typing|collections|asyncio|logging|hashlib|enum|dataclasses|io|math|random|string|copy|itertools|functools|decimal|fractions|statistics|textwrap|unicodedata|email|http|urllib|xml|sqlite3|csv|configparser|html|zoneinfo|threading|multiprocessing|queue|socket|ssl|select|subprocess|tempfile|shutil|glob|fnmatch|pickle|gzip|zipfile|tarfile|weakref|types|inspect|pkgutil|importlib|warnings|traceback|gc|sysconfig|platform|unittest|doctest|pdb|profile|pstats|timeit|resource|tracemalloc|dis|compile|ast|parser|symbol|token|keyword|tokenize|tabnanny|py_compile|importlib|site|user|builtins))\b'

            has_issue = False
            for i in range(1, len(imports)):
                curr = imports[i]['content']
                prev = imports[i-1]['content']

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ†ç»„
                if (curr.startswith('from app.') and not prev.startswith(('from app.', 'import app.'))) or \
                   (prev.startswith('from app.') and not curr.startswith(('from app.', 'import app.'))):
                    has_issue = True
                    break

            if has_issue:
                issues.append({
                    'type': 'import_order',
                    'message': 'Import statements should be grouped (stdlib, third-party, local)'
                })

        return issues

    def _calculate_quality_score(self, metrics: Dict[str, int], issues: List[Dict]) -> float:
        """è®¡ç®—ä»£ç è´¨é‡åˆ†æ•°"""
        score = 100.0

        # æ‰£åˆ†é¡¹
        if metrics['functions'] > 0:
            docstring_ratio = metrics['docstrings'] / metrics['functions']
            score -= (1 - docstring_ratio) * 20  # æ–‡æ¡£å­—ç¬¦ä¸²è¦†ç›–ç‡

        if metrics['functions'] > 0:
            type_annotation_ratio = metrics['type_annotations'] / (metrics['functions'] * 2)  # å‡è®¾æ¯ä¸ªå‡½æ•°å¹³å‡2ä¸ªæ³¨è§£
            score -= (1 - min(type_annotation_ratio, 1.0)) * 15  # ç±»å‹æ³¨è§£è¦†ç›–ç‡

        # é—®é¢˜æ‰£åˆ†
        score -= len(issues) * 2  # æ¯ä¸ªé—®é¢˜æ‰£2åˆ†

        return max(0, score)

    def analyze_project(self, paths: List[Path]) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        all_files = []

        for path in paths:
            if path.is_file() and path.suffix == '.py':
                all_files.append(path)
            elif path.is_dir():
                all_files.extend(path.rglob('*.py'))

        self.results['total_files'] = len(all_files)
        self.results['python_files'] = len(all_files)

        print(f"Analyzing {len(all_files)} Python files...")

        for file_path in all_files:
            file_result = self.analyze_file(file_path)
            if file_result:
                all_files.append(file_result)

                # ç´¯è®¡æŒ‡æ ‡
                metrics = file_result['metrics']
                for key, value in metrics.items():
                    self.results['metrics'][key] += value

                # ç´¯è®¡é—®é¢˜
                for issue in file_result['issues']:
                    self.results['issues'][issue['type']].append(issue)

        # è®¡ç®—è¦†ç›–ç‡
        if self.results['metrics']['functions'] > 0:
            self.results['docstring_coverage'] = (self.results['metrics']['docstrings'] /
                                                self.results['metrics']['functions']) * 100

        return self.results

    def generate_report(self) -> str:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š")
        report.append("Code Quality Analysis Report")
        report.append("=" * 60)

        # åŸºæœ¬ç»Ÿè®¡
        report.append(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        report.append(f"   Pythonæ–‡ä»¶æ•°: {self.results['python_files']}")
        report.append(f"   æ€»ä»£ç è¡Œæ•°: {self.results['metrics']['code_lines']:,}")
        report.append(f"   å‡½æ•°æ•°é‡: {self.results['metrics']['functions']}")
        report.append(f"   ç±»æ•°é‡: {self.results['metrics']['classes']}")
        report.append(f"   æ–‡æ¡£å­—ç¬¦ä¸²è¦†ç›–ç‡: {self.results['docstring_coverage']:.1f}%")

        # é—®é¢˜ç»Ÿè®¡
        report.append(f"\nâš ï¸  å‘ç°çš„é—®é¢˜:")
        total_issues = sum(len(issues) for issues in self.results['issues'].values())
        report.append(f"   æ€»é—®é¢˜æ•°: {total_issues}")

        for issue_type, issues in self.results['issues'].items():
            if issues:
                report.append(f"   {issue_type}: {len(issues)}")

        # å»ºè®®
        report.append(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")

        if self.results['docstring_coverage'] < 80:
            report.append("   - å¢åŠ å‡½æ•°å’Œç±»çš„æ–‡æ¡£å­—ç¬¦ä¸²")

        if any('type' in issue_type for issue_type in self.results['issues'].keys()):
            report.append("   - æ·»åŠ ç±»å‹æ³¨è§£ä»¥æé«˜ä»£ç å¯è¯»æ€§")

        if 'import_order' in self.results['issues']:
            report.append("   - æ•´ç†å¯¼å…¥è¯­å¥çš„é¡ºåº")

        if 'long_line' in self.results['issues']:
            report.append("   - å°†è¿‡é•¿çš„ä»£ç è¡Œæ‹†åˆ†ä¸ºå¤šè¡Œ")

        report.append("=" * 60)

        return '\n'.join(report)


def main():
    """ä¸»å‡½æ•°"""
    analyzer = CodeQualityAnalyzer()

    # åˆ†æä¸»è¦ç›®å½•
    paths_to_analyze = [
        project_root / "app",
        project_root / "celery_worker",
        project_root / "scripts"
    ]

    results = analyzer.analyze_project(paths_to_analyze)

    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report()
    print(report)

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = project_root / "code_quality_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


if __name__ == "__main__":
    main()